{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git workflow\n",
    "\n",
    "1. create or clone (for github use) a repo\n",
    "2. add files\n",
    "3. commit\n",
    "4. when you are ready to make changes to those files, create a new branch and go to it\n",
    "5. make changes to files\n",
    "6. commit the changes\n",
    "7. go back to the master branch\n",
    "8. When you are satified with the changes, merge them in\n",
    "9. If using Github, push the local changes to the site \n",
    "\n",
    "Commands, in order:\n",
    "1. `git clone <repo address>` (for github repos) or `git init` (for local folders)\n",
    "3. `git add <filenames>`, then `git commit -m 'your message'`\n",
    "4. `git checkout -b <branchname>` (the -b flag says to first create and then checkout the branch)\n",
    "6. `git add <filenames>`, then `git commit -m 'your message'`\n",
    "7. `git checkout master`\n",
    "8. `git merge master branchname`\n",
    "9. `git push origin master`\n",
    "\n",
    "# Exercise\n",
    "Run through this process once, starting by creating a github repo.\n",
    "\n",
    "# More on Difference Between Means\n",
    "\n",
    "There are a few hypotheses that need to be satisfied for the hypothesis tests discussed in last week's module to be valid. First, you have to have enough data, but not too much, and ideally we would like to see that the data are roughly normally distributed. We can do that with boxplots, histograms, or quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, norm\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "% matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example with datasets of the size for which these tests were designed. Suppose the values below are quiz scores for two different classes (number correct, out of 20 possible). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_a = np.array([15, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 17, 18, 20, 20, 12, 13])\n",
    "class_b = np.array([16, 18, 13, 13, 17, 20, 19, 18, 15, 20, 17, 18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Calculate the sample means. Are they the same or are they different?\n",
    "2. Calculate the sample standard deviations. Are they the same or are they different?\n",
    "3. Conduct a two-sample $t$-test to see if the means are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "1. Plot the distributions (either boxplot, histogram, or kde plot). Do they look normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Change the values in the second dataset slightly to make the differences statistically significant at the 5% level (that is, get the $p$-value below 0.05). What is the mean difference now? Can you make the mean difference smaller while keeping $p < 0.05$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problems with this framework\n",
    "\n",
    "This type of testing breaks down with large datasets. The reason is that part of the calculation of $t$ is dividing by the square root of the sample size, which with a large sample produces a $t$-statistic that is enormous. Another way of saying that is that this testing strategy will always assert that the means are different if the sample sizes are large enough. To see this, let's go back to the complaints data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complaints = pd.read_csv('/Volumes/data/311/311_1e5.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complaints['Created Date'] = pd.to_datetime(complaints['Created Date'])\n",
    "complaints['Closed Date'] = pd.to_datetime(complaints['Closed Date'])\n",
    "complaints['Duration'] = complaints['Closed Date'] - complaints['Created Date'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Compare the average complaint duration by borough using a $t$-test. Are the differences statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Pick two different complaints. Note the number of complaints of each type. Compare the average duration for the two types of complaints. Are the differences statistically significant? What if you pick complaints with roughly 50 occurrences each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Pick a complaint and two different boroughs. Compare the average duration of that complaint by borough. Is the difference statistically significant? How many of this type of complaint are there? What happens if you pick a complaint that occurs roughly 50 times in the dataset? One that occurs more than 2000 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "'Multiple comparison' refers to testing the same thing in different contexts over and over. This leads to trouble: suppose our threshold for significance is 5% (1/20). If we perform 20 tests, by pure chance we expect to see a result that looks significant, but isn't. Read this [xkcd comic](https://xkcd.com/882/). Note that we just did some multiple comparisons, and in fact should have followed a slightly modified procedure. More to come this week - see the [F-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html) for more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:icicles]",
   "language": "python",
   "name": "conda-env-icicles-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
